{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Using device: cpu\n",
      "Dataset sample:\n",
      "                 id                                       comment_text  toxic  \\\n",
      "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
      "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
      "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
      "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
      "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
      "\n",
      "   severe_toxic  obscene  threat  insult  identity_hate  \n",
      "0             0        0       0       0              0  \n",
      "1             0        0       0       0              0  \n",
      "2             0        0       0       0              0  \n",
      "3             0        0       0       0              0  \n",
      "4             0        0       0       0              0  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from transformers import BertForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import transformers\n",
    "import random\n",
    "\n",
    "# -----------------------\n",
    "# Set random seed for reproducibility\n",
    "seed_val = 42\n",
    "torch.manual_seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "random.seed(seed_val)\n",
    "\n",
    "# -----------------------\n",
    "# Patch torch.get_default_device in transformers\n",
    "def patched_get_default_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def patch_transformers():\n",
    "    import transformers.trainer\n",
    "    if hasattr(transformers.trainer, 'torch'):\n",
    "        original_torch_generator = transformers.trainer.torch.Generator\n",
    "        class PatchedGenerator(original_torch_generator):\n",
    "            def __new__(cls, *args, **kwargs):\n",
    "                instance = super(PatchedGenerator, cls).__new__(cls, *args, **kwargs)\n",
    "                instance.get_default_device = patched_get_default_device\n",
    "                return instance\n",
    "        transformers.trainer.torch.Generator = PatchedGenerator\n",
    "\n",
    "patch_transformers()\n",
    "\n",
    "# -----------------------\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------\n",
    "# Load the dataset\n",
    "dataset_path = 'datasets/train.csv'\n",
    "df = pd.read_csv(dataset_path)\n",
    "print(\"Dataset sample:\")\n",
    "print(df.head())\n",
    "\n",
    "# -----------------------\n",
    "# Task 2: Define distillation functions\n",
    "def distill_bert_weights_odd(teacher: nn.Module, student: nn.Module) -> nn.Module:\n",
    "    if hasattr(teacher, \"bert\"):\n",
    "        teacher_layers = teacher.bert.encoder.layer\n",
    "        student_layers = student.bert.encoder.layer\n",
    "        for i in range(len(student_layers)):\n",
    "            student_layers[i].load_state_dict(teacher_layers[2*i + 1].state_dict())\n",
    "    return student\n",
    "\n",
    "def distill_bert_weights_even(teacher: nn.Module, student: nn.Module) -> nn.Module:\n",
    "    if hasattr(teacher, \"bert\"):\n",
    "        teacher_layers = teacher.bert.encoder.layer\n",
    "        student_layers = student.bert.encoder.layer\n",
    "        for i in range(len(student_layers)):\n",
    "            student_layers[i].load_state_dict(teacher_layers[2*i].state_dict())\n",
    "    return student\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 159571/159571 [00:18<00:00, 8701.06 examples/s] \n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\bitsandbytes\\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1609: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 221,184 || all params: 67,177,730 || trainable%: 0.3293\n",
      "Starting LoRA model training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 06:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.307000</td>\n",
       "      <td>0.361396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.320800</td>\n",
       "      <td>0.355269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.355224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.314200</td>\n",
       "      <td>0.354657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.383000</td>\n",
       "      <td>0.353245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\utils\\other.py:716: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1e2c84ac-f5ba-4048-83d4-6c876f37ad0f)') - silently ignoring the lookup for the file config.json in bert-base-uncased.\n",
      "  warnings.warn(\n",
      "c:\\Users\\VUONGLOCTRUONG\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\peft\\utils\\save_and_load.py:246: UserWarning: Could not find a config file in bert-base-uncased - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=250, training_loss=0.33424978637695313, metrics={'train_runtime': 373.9381, 'train_samples_per_second': 10.697, 'train_steps_per_second': 0.669, 'total_flos': 133146875904000.0, 'train_loss': 0.33424978637695313, 'epoch': 5.0})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 3: Preprocess dataset\n",
    "teacher_model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"comment_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    tokenized_inputs[\"labels\"] = [int(x) for x in examples[\"toxic\"]]\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Convert to Hugging Face Dataset and take a subset for faster iteration\n",
    "dataset = Dataset.from_pandas(df)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "subset_size = 1000  # Adjust as needed\n",
    "tokenized_dataset = tokenized_dataset.select(range(min(len(tokenized_dataset), subset_size)))\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"comment_text\", \"id\", \"obscene\", \"identity_hate\", \"threat\", \"severe_toxic\", \"insult\"])\n",
    "tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"], device=device)\n",
    "\n",
    "# Split dataset into training and evaluation sets\n",
    "train_size = int(0.8 * len(tokenized_dataset))\n",
    "train_dataset = tokenized_dataset.select(range(train_size))\n",
    "eval_dataset = tokenized_dataset.select(range(train_size, len(tokenized_dataset)))\n",
    "\n",
    "# -----------------------\n",
    "# Task 2: Odd Layer vs Even Layer Training\n",
    "# Initialize teacher model\n",
    "teacher_model = BertForSequenceClassification.from_pretrained(teacher_model_name, num_labels=2)\n",
    "\n",
    "# Configure the student model with 6 layers\n",
    "student_config = teacher_model.config\n",
    "student_config.num_hidden_layers = 6\n",
    "\n",
    "# Initialize student models for odd and even layer distillation\n",
    "student_model_odd = BertForSequenceClassification(student_config).to(device)\n",
    "student_model_even = BertForSequenceClassification(student_config).to(device)\n",
    "\n",
    "# Distill the models from the teacher\n",
    "student_model_odd = distill_bert_weights_odd(teacher_model, student_model_odd)\n",
    "student_model_even = distill_bert_weights_even(teacher_model, student_model_even)\n",
    "\n",
    "# -----------------------\n",
    "# Task 3: LoRA (Low-Rank Adaptation)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"query\", \"key\", \"value\"],\n",
    ")\n",
    "student_model_lora = get_peft_model(BertForSequenceClassification(student_config).to(device), lora_config)\n",
    "student_model_lora.print_trainable_parameters()\n",
    "\n",
    "# -----------------------\n",
    "# Training arguments with mixed precision and adjusted hyperparameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,            # Increased epochs for better convergence\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    remove_unused_columns=False,\n",
    "    no_cuda=not torch.cuda.is_available(),\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# -----------------------\n",
    "# Trainer for LoRA model\n",
    "trainer_lora = Trainer(\n",
    "    model=student_model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA model training...\")\n",
    "trainer_lora.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxic probability on eval set: 0.609\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxic probability on eval set: 0.651\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average toxic probability on eval set: 0.083\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š **Model Evaluation Results**\n",
      "ðŸ”¹ **Odd Layer Model**  â†’ Loss: 0.8867, Accuracy: 0.1150, F1-score: 0.0237\n",
      "ðŸ”¹ **Even Layer Model** â†’ Loss: 0.9866, Accuracy: 0.1150, F1-score: 0.0237\n",
      "ðŸ”¹ **LoRA Model**      â†’ Loss: 0.3532, Accuracy: 0.8850, F1-score: 0.8310\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Evaluation and Analysis\n",
    "def evaluate_model(model, dataset, threshold=0.5):\n",
    "    \"\"\"Evaluate the model: compute accuracy, F1-score, and optionally adjust threshold using softmax probabilities.\"\"\"\n",
    "    trainer = Trainer(model=model, args=training_args, eval_dataset=dataset)\n",
    "    predictions = trainer.predict(dataset)\n",
    "    logits = predictions.predictions\n",
    "    labels = predictions.label_ids\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "    \n",
    "    # Optionally use thresholding on toxic probability (assumes label 1 = Toxic)\n",
    "    preds = (probs[:, 1] >= threshold).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    \n",
    "    # Debug: print average toxic probability for evaluation set\n",
    "    avg_toxic_prob = probs[:, 1].mean()\n",
    "    print(f\"Average toxic probability on eval set: {avg_toxic_prob:.3f}\")\n",
    "    \n",
    "    return acc, f1\n",
    "\n",
    "# Evaluate models\n",
    "acc_odd, f1_odd = evaluate_model(student_model_odd, eval_dataset)\n",
    "acc_even, f1_even = evaluate_model(student_model_even, eval_dataset)\n",
    "acc_lora, f1_lora = evaluate_model(student_model_lora, eval_dataset)\n",
    "\n",
    "# Also evaluate loss using Trainer.evaluate\n",
    "trainer_odd = Trainer(model=student_model_odd, args=training_args, eval_dataset=eval_dataset)\n",
    "trainer_even = Trainer(model=student_model_even, args=training_args, eval_dataset=eval_dataset)\n",
    "trainer_lora_model = Trainer(model=student_model_lora, args=training_args, eval_dataset=eval_dataset)\n",
    "\n",
    "loss_odd = trainer_odd.evaluate().get(\"eval_loss\")\n",
    "loss_even = trainer_even.evaluate().get(\"eval_loss\")\n",
    "loss_lora = trainer_lora_model.evaluate().get(\"eval_loss\")\n",
    "\n",
    "print(\"\\nðŸ“Š **Model Evaluation Results**\")\n",
    "print(f\"ðŸ”¹ **Odd Layer Model**  â†’ Loss: {loss_odd:.4f}, Accuracy: {acc_odd:.4f}, F1-score: {f1_odd:.4f}\")\n",
    "print(f\"ðŸ”¹ **Even Layer Model** â†’ Loss: {loss_even:.4f}, Accuracy: {acc_even:.4f}, F1-score: {f1_even:.4f}\")\n",
    "print(f\"ðŸ”¹ **LoRA Model**      â†’ Loss: {loss_lora:.4f}, Accuracy: {acc_lora:.4f}, F1-score: {f1_lora:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models using torch.save (state_dict)\n",
    "torch.save(student_model_odd.state_dict(), \"model/student_model_odd.pth\")\n",
    "torch.save(student_model_even.state_dict(), \"model/student_model_even.pth\")\n",
    "torch.save(student_model_lora.state_dict(), \"model/student_model_lora.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
